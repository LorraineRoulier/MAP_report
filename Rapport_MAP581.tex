\documentclass[a4paper,12pt]{article}

\usepackage[latin1]{inputenc} % accents
\usepackage[T1]{fontenc}      % caractères français
\usepackage{geometry}         % marges
\usepackage{lmodern}
\usepackage[french]{babel}
\usepackage{url,csquotes}
\usepackage[hidelinks,hyperfootnotes=false]{hyperref}
\usepackage{graphicx}
\usepackage[titlepage]{polytechnique}
\usepackage{textcomp}
\usepackage{float}
\usepackage{enumerate}
\usepackage{enumitem}%textbullets
 \frenchbsetup{StandardLists=true}%textbullets
 \usepackage{soul}
 \usepackage{color}
 \usepackage{amsfonts}
 \usepackage{amsmath}
 
 
\title{Low Dimensional Embedding of Environmental Variables}      % renseigne le titre
\subtitle{EA MAP581}
\author{Flore Martin and Lorraine Roulier}           %   "   "   l'auteur
%\date{\today}           %   "   "   la future date de parution
\renewcommand{\thesection}{\arabic{section}}

\definecolor{bleu}{rgb}{0.5, 1.0, 1.0}
\newcommand{\hlb}[1]{\sethlcolor{bleu}\hl{#1}}


\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}

Climate data amounts very quickly to a lot of unused data. In a day, we can collect temperature, pressure, wind data all over the world with satellites, even hourly. Our project was two sided. First, we familiarized with various dimension reduction techniques, then we attempted to show that the geographical position of a point on the planet - e.g. it's latitude and longitude - were embedded in the climate data one could gather on it. 

Dimension reduction techniques can be divided in two classes, linear dimension reduction and non linear dimension reduction. However, in all methods, the main goal is to figure out a similarity function between vectors. Such a function will then enable to sort the dataset into classes of vectors with similar features, which would have been more intricate with the initial dataset. 
We used a set of datasets we found on the NASA website, that gathered various means on climate variables over 22 years at every given latitude and longitude. These variables are gathered in the table below

\begin{figure}[H]
 \begin{center}
	
	\begin{tabular}{|c|c|c|c|c|c|c|}
		 \hline    
	  	Latitude & Longitude &Temperature & Pressure&  Relative Humidity& Wind Speed& Radiation\\ 
		
		& & °$C$ & $kPa$ & $ \%$ & $m/s$ & $kWh/m^{2}/day$ \\ 
		\hline
		  \end{tabular}
\end{center}
\caption{First lign of our dataset}
\end{figure}

The latitude parameter varies from -90 to 89 and the longitude parameter varies from -180 to 179. the negative values are for the south hemisphere, the positive ones for the north. Depending on the running time of the method, we did not compute the dimension reduction with the 64800 lines, but with a subset. 
The subset is often a slice of longitudes containing all latitudes, as we assumed that the critical parameter to differenciate climate data was the latitude. 

\begin{figure}[H]
 \begin{center}
	\includegraphics[scale = 4]{latlon.jpg}
\end{center}
\caption{An example of subset in grey}
\end{figure}

We classified the data according to the latitude, creating five classes listed in the table below: 
\begin{figure}[H]
 \begin{center}
	
	\begin{tabular}{|c|c|c|c|c|c|}
		 \hline    
	  	& North &Temperate north & Equator &  Temperate South & South\\ 
		\hline
		Latitudes & 90 to 66 & 66 to 23 & 23 to -23 & -23 to -66 & -66 to -90 \\
		\hline
		  \end{tabular}
\end{center}
\caption{First lign of our dataset}
\end{figure}

\section{Principal Component Analysis - PCA}
	\subsection{Method}
	
		Principal Component Analysis detects tendencies in the data by maximizing the variance of the dataset matrix. This yields an orthonormal matrix that can be diagonalized. The largest eigenvalues point to the eigenvectors that contain the most information about the dataset. 
		
		Let $ X \in \mathbb{R}^{d \times n} $ be our dataset, PCA maximizes the following equation :
%%
\[ \| X - MM^{T}X \|^{2} \]
subject to $ M \in \mathcal{O}^{d \times r} $ where $ r<d $.

	\subsection{Results}
		\subsubsection{Two dimensions}
		
		We first implemented PCA and ran it with only two principal components, which yielded the following graph for the whole dataset. The associated eigenvectors were 
		\begin{align*}
			y_{1} &= \begin{bmatrix} 
					-0.94220902\\
					 -0.31329122 \\
					  0.10889051\\
					   -0.04573547\\
					     0.01191187
				      \end{bmatrix}
		\end{align*} 
		and 
		\begin{align*}
			y_{2} &= \begin{bmatrix} 
				      0.02637512\\
				       -0.40428947\\
				        -0.9117768 \\  
				        0.04144017\\
				         -0.05291649
				      \end{bmatrix}
		\end{align*} 
		
		This enables us to understand the meaning of these vectors. $ y_{1} $ is mostly related to a decreasing temperature and pressure, and $ y_{2} $ represents decreasing humidity and pressure. 
		
\begin{figure}[H]
 \begin{center}
	\includegraphics[scale = 0.5]{pca2.png}
\end{center}
\caption{PCA with two components}
\end{figure}

We can see on the graph that even if there is a strong dispersion, equator values are located at higher temperatures. On the contrary, north and south pole values are located at lower temperatures. Green and red classes overlap as these to categories have similar climate conditions. 

Although it is an understandable figure, this is not satisfying. We plot the eigenvalues to see their relative importance in the dimension reduction. 

\begin{figure}[H]
 \begin{center}
	\includegraphics[scale = 0.8]{eigen.png}
\end{center}
\caption{PCA with two components}
\end{figure}

\section{Kernel Principal Component Analysis}
	\subsection{Method}
	\subsection{Results}
\section{Multidimensional Scaling - MDS}
	\subsection{Method}
	Multidimensional scaling is a non-linear approach to reduce dimension of a dataset. The principle is quite different from PCA: given a matrix of distance or "`dissimilarity"', MDS aims at reconstructing a map preserving at best the distances between datas. In our example, the MDS algorithm aims to place each object in 2-dimensional space such that the between-object distances are preserved as well as possible.
	Given a distance matrix D of dimension n*p (p=5 for us) , MDS attempts to find the datapoints $y_1$,... $y_t$ in dimension d<p (d=2 for us) that minimizes:
	
	
		 \[\sum\limits_{i=1}^{t}[\sum\limits_{j=1}^{t}d_{ij}^{(X)}-d_{ij}^{(Y)}]
			\]
		

	with $d_{ij}^{(X)}$ and $d_{ij}^{(Y)}$ respectively the euclidean distance between pairwise i and j in the original  n*p matrix $D^{(X)}$  and in the computed n*d matrix $D^{(Y)}$.
	
	As the distance matrix $D^{(X)}$  can be converted into a kernel matrix of inner products $X^TX$ by 
	\[ X^TX=-1/2HD^{(X)}H
		\]

H can be written as $H=I-ee^T$ and $e$ is the vector of ones. Thus, ??? becomes:
	\[\sum\limits_{i=1}^{t}\sum\limits_{j=1}^{t} x_{i}^{T}x_{j}-y_{i}^{T}y_{j}
		\]
	It can be shown that the solution is $Y=\Lambda^{1/2}V^T$ with $V$ the eigenvectors of $X^TX$ of the top d eigenvalues present in $\Lambda$.
	The only and main parameter of this method is the norm used to calculate the distance matrix. By default we use the euclidean norm here. 
	
	
	\subsection{Results}
	
	Figure 6 shows the results of MDS with our previous dataset. We use the same grandeurs (temperature, radiation, wind speed, humidity and pressure) but not the same number of lines. As MDS was extremeley low and seemed to use a lot of memory, we could not run the programm for more than 600 rows. Here are the result for 546 rows: we filtered our dataset with latitudes < 0 (ie. south hemisphere) and longitude between 0 and 5 degrees. 
	
	\begin{figure}[H]
 \begin{center}
	\includegraphics[scale = 1]{graphMDS.png}
\end{center}
\caption{results of MDS in 2D}
\end{figure}
	
	One indicator to estimate of the reconstruction is accurate is the 'stress'. It is roughly the square difference between the final distance of two items in the MDS model and the true distance between them. The exact formula is: 
		
			\[ s = \sqrt{\frac{\sum (d_ij-d(i,j))?}{\sum d_ij?} 
		\]
		
		The aim of MDS is to minimize this stress. Figure 7 presents the value of stress depending on the number of lines. We can see it is relatively low (<0,002) for more than 500 rows.
		
	\begin{figure}[H]
 \begin{center}
	\includegraphics[scale = 1]{stressMDSnblines.png}
\end{center}
\caption{stress as a function of number of rows}
\end{figure}

\section{Isomap}
	\subsection{Method}
	
	Isomap is a low dimensional embedding method similar to MDS. The difference is, distance is not computed with euclidean norm but with geodesic distance. 
	\subsection{Results}
	
	As the main parameter of the method is the number of nearest neighbors taken into account, we present infigure 8 the result of isomap for number of neighbour from 1 to 11 neighbors. 
	
	\begin{figure}[H]
 \begin{center}
	\includegraphics[scale = 0.5]{graphiso.png}
\end{center}
\caption{results of isomap depending on the number of neighbors considered from 2 to 12}
\end{figure}

It is obvious that the number of neighbours considered strongly influence the 2D distribution of our dataset. To know which graph is the most accurate, we refer to the 'reconstruction error'. We computed this reconsruction error with respect to the number of neighbours considered. The results are presented in figure ??

We also computed the error reconstruction as a function of the number of rows in figure 9. This error keeps increasing with the number or rows, so isomap accuracy seems to be limited by the size of the dataset.

\begin{figure}[H]
 \begin{center}
	\includegraphics[scale = 0.8]{isoerrorneighbour.png}
\end{center}
\caption{reconstruction error as a function of number of neighbours considered}
\end{figure}

It is interesting to see that from eight neighbours, error seems to stabilize around 2,5. Indeed, we can see with graphs of figure 8 that from eight neighbours, graphs look quite all the same. Those are the most 'accurate' representations.
	
\section{Comparing the different methods}
\section{Conclusion}
\section{Bibliography}


\end{document}
